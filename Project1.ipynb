{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcbb35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "seed(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc2e00",
   "metadata": {},
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import project1; import importlib; importlib.reload(project1); from project1.project1 import make_design_matrix\n",
    "N=5\n",
    "x, y = np.meshgrid(np.linspace(0,1,N),np.linspace(0,1,N))\n",
    "xvec  = np.array([x, y])\n",
    "X = make_design_matrix(xvec, p=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e59179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "N=5\n",
    "x, y = np.meshgrid(np.linspace(0,1,N),np.linspace(0,1,N))\n",
    "xvec  = np.array([x, y])\n",
    "xi = {\"x%i\"%i : xvec[i].ravel() for i in range(len(xvec))}\n",
    "keys = [key for key in xi.keys()]\n",
    "comb = []\n",
    "p = 2\n",
    "for p in range(1,p+1):\n",
    "    comb += [x for x in combinations_with_replacement(keys, p )]\n",
    "print(comb)\n",
    "X = make_design_matrix(xvec, p=2)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X(x, y, n ):\n",
    "\tif len(x.shape) > 1:\n",
    "\t\tx = np.ravel(x)\n",
    "\t\ty = np.ravel(y)\n",
    "\n",
    "\tN = len(x)\n",
    "\tl = int((n+1)*(n+2)/2)\t\t# Number of elements in beta\n",
    "\tX = np.ones((N,l))\n",
    "\n",
    "\tfor i in range(1,n+1):\n",
    "\t\tq = int((i)*(i+1)/2)\n",
    "\t\tfor k in range(i+1):\n",
    "\t\t\tX[:,q+k] = (x**(i-k))*(y**k)\n",
    "\n",
    "\treturn X\n",
    "\n",
    "x, y = np.meshgrid(np.linspace(0,1,N),np.linspace(0,1,N))\n",
    "X_lecturer = create_X(x = x, y = y, n = 2)\n",
    "np.sum(X_lecturer == X) == len(X.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd837fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_lecturer)\n",
    "X_lecturer[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lecturer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93700a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_lecturer, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1753aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lecturer[:,1] - np.mean(X_lecturer[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42fe19",
   "metadata": {},
   "source": [
    "# Time for generating design matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac039d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model_complexity = 5\n",
    "xvec = np.array([x,y])\n",
    "for p in range(1,model_complexity+1):\n",
    "\n",
    "    tik = time.time()\n",
    "    X = make_design_matrix(xvec = xvec, p = p)\n",
    "    #X = create_X(x = x1, y=y1, n = p)\n",
    "    tok = time.time()\n",
    "    print(tok-tik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d781dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model_complexity = 5\n",
    "xvec = np.array([x1,y1])\n",
    "for p in range(1,model_complexity+1):\n",
    "    tik = time.time()\n",
    "    #X = make_design_matrix(xvec = xvec, p = p)\n",
    "    X = create_X(x = x, y=y, n = p)\n",
    "    tok = time.time()\n",
    "    print(tok-tik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc24fe",
   "metadata": {},
   "source": [
    "# Simple 1D test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9813ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import project1.project1;import importlib; importlib.reload(project1.project1);from project1.project1 import *\n",
    "x = np.arange(0,1,0.05)\n",
    "def test_func(**kwargs):\n",
    "    x = kwargs['x0']\n",
    "    return 5*x**2 + x\n",
    "\n",
    "xvec  = np.array([x])\n",
    "X = make_design_matrix(xvec = xvec, p = 5)\n",
    "z = test_func(**{'x0':x})\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "znoisy_centered = np.array([[x] for x in znoisy - np.mean(znoisy)])\n",
    "A = np.linalg.pinv(X.T@X)@X.T\n",
    "betahat = A@znoisy_centered\n",
    "znoisy_tilde = X@betahat\n",
    "plt.plot(x, znoisy_centered, linestyle = 'dashed')\n",
    "plt.plot(x, znoisy_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import project1.project1;import importlib; importlib.reload(project1.project1);from project1.project1 import *\n",
    "x = np.arange(0,1,0.05)\n",
    "xvec = np.array([x])\n",
    "z = test_func(**{'x0':x})\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "znoisy_centered = znoisy - np.mean(znoisy)\n",
    "X = make_design_matrix(xvec = xvec, p = 5)\n",
    "znoisy_tilde, betahat = ols_fp_wo_split(X = X, y = znoisy_centered)\n",
    "plt.plot(x, znoisy_centered, linestyle = 'dashed')\n",
    "plt.plot(x, znoisy_tilde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22291755",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import project1.project1;import importlib; importlib.reload(project1.project1);from project1.project1 import *\n",
    "x, y = np.arange(0,1,0.1), np.arange(0,1,0.1)\n",
    "x, y = np.meshgrid(x,y)\n",
    "def test_func_2(**kwargs):\n",
    "    x = kwargs['x0']\n",
    "    y = kwargs['x1']\n",
    "    return 5*x**2 + x + 5*y**2 + y + np.exp(-x**2) +np.exp(-x*y) + np.exp(-y) \n",
    "xvec = np.array([x, y])\n",
    "X = make_design_matrix(xvec = xvec, p = 5)\n",
    "z = test_func_2(**{'x%i'%i: xvec[i] for i in range(len(xvec))})\n",
    "\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "znoisy_centered = znoisy - np.mean(znoisy)\n",
    "znoisy_tilde, betahat = ols_fp_wo_split(X = X, y = znoisy_centered.ravel())\n",
    "znoisy_tilde = znoisy_tilde.reshape(x.shape)\n",
    "znoisy = znoisy.reshape(x.shape)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={'projection':'3d'}, figsize=(10,10))\n",
    "ax.plot_surface(x,y,znoisy_tilde + np.mean(znoisy), cmap=cm.viridis)\n",
    "ax.plot_surface(x,y,z, cmap=cm.coolwarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a1fa1",
   "metadata": {},
   "source": [
    "# b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd45e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib; import project1; importlib.reload(project1)\n",
    "from project1.project1 import *\n",
    "x, y = np.meshgrid(np.arange(0,1,0.05),np.arange(0,1,0.05))\n",
    "xvec = np.array([x,y])\n",
    "z = FrankeFunction(**{'x%i'%i: xvec[i] for i in range(len(xvec))})\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "znoisy_centered = znoisy - np.mean(znoisy)\n",
    "for p in range(1,6):\n",
    "    X = make_design_matrix(xvec = xvec, p = p)\n",
    "    znoisy_tilde, betahat = ols_fp_wo_split(X = X, y = znoisy_centered.ravel())\n",
    "znoisy_tilde = (znoisy_tilde + np.mean(znoisy)).reshape(x.shape)\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={'projection':'3d'}, figsize=(10,10))\n",
    "ax.plot_surface(x,y,znoisy_tilde, cmap=cm.viridis)\n",
    "ax.plot_surface(x,y,z, cmap=cm.coolwarm)\n",
    "ax.view_init(20,45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib; import project1; importlib.reload(project1)\n",
    "from project1.project1 import *\n",
    "test_size = 0.3\n",
    "for N in [50,100, 500]:\n",
    "    x, y = np.meshgrid(np.linspace(0,1,N),np.linspace(0,1,N))\n",
    "    xvec = np.array([x,y])\n",
    "    maxdeg = 10\n",
    "    mses_train = np.zeros((maxdeg))\n",
    "    mses_test = np.zeros((maxdeg))\n",
    "    Rs_train = np.zeros((maxdeg))\n",
    "    Rs_test = np.zeros((maxdeg))\n",
    "\n",
    "    betas = []\n",
    "    z = FrankeFunction(**{'x%i'%i: xvec[i] for i in range(len(xvec))})\n",
    "    noise = np.random.normal(0,1,size=z.shape)\n",
    "    znoisy = z + noise\n",
    "    for p in range(1,maxdeg+1):\n",
    "        print(p)\n",
    "        #X = make_design_matrix(xvec = xvec, p = p)\n",
    "        X = create_X(x = xvec[0].ravel(), y = xvec[1].ravel(), n = p)\n",
    "\n",
    "        Xtrain, Xtest, znoisy_train, znoisy_test = train_test_split(X, znoisy.ravel(), \n",
    "                                                                    **{'random_state' : 42, 'test_size':test_size})\n",
    "        scale = 1/np.sqrt(np.sum(znoisy_train**2))\n",
    "        ztilde_train, betahat = ols_fp_wo_split(X = Xtrain, y = scale*(znoisy_train-np.mean(znoisy_train)))\n",
    "\n",
    "        ztilde_train = (1/scale)*ztilde_train + np.mean(znoisy_train)\n",
    "        ztilde_test = (1/scale)*Xtest@betahat + np.mean(znoisy_train)\n",
    "        betas.append(betahat)\n",
    "\n",
    "        mses_train[p-1] = MSE(y = znoisy_train,ytilde = ztilde_train)\n",
    "        mses_test[p-1] = MSE(y = znoisy_test,ytilde = ztilde_test )\n",
    "        Rs_train[p-1] = Rscore(y = znoisy_train, ytilde = ztilde_train)\n",
    "        Rs_test[p-1] = Rscore(y = znoisy_test, ytilde = ztilde_test)\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "    polydeg = np.arange(maxdeg)+1\n",
    "    fig.suptitle(\"$N=%i$, testsize $%.1f$ percentage\"%(N, test_size*100))\n",
    "    axs[0].plot(polydeg, mses_train, ls=\"--\", marker='o', label=\"Train\")\n",
    "    axs[0].plot(polydeg, mses_test, marker='o', label=\"Test\")\n",
    "    axs[0].set_ylabel(\"MSE\")\n",
    "    axs[1].plot(polydeg, Rs_train, marker='o', ls = \"--\", label = \"Train\")\n",
    "    axs[1].plot(polydeg, Rs_test, marker='o', label = \"Test\")\n",
    "\n",
    "    axs[1].set_ylabel(\"$R^2$ score\")\n",
    "    [ax.set_xlabel(\"Polynomial degree $p$\") for ax in axs]\n",
    "    [ax.set_xticks(polydeg, polydeg) for ax in axs]\n",
    "    [ax.legend() for ax in axs]\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfe9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "beta_matrix = np.ones((maxdeg, len(betas[-1])))*np.nan\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(maxdeg):\n",
    "    for j in range(len(betas[i])):\n",
    "        beta_matrix[i,j] = betas[i][j]\n",
    "[plt.plot(np.arange(maxdeg) + 1, beta_matrix[:,i], label=\"$\\\\beta%i$\"%i) for i in range(beta_matrix.shape[-1])]\n",
    "plt.ylabel(\"$\\\\beta$\")\n",
    "plt.xlabel(\"Polynomial degree $p$\")\n",
    "plt.legend(ncol = 2, bbox_to_anchor=(1,.5,.15,.5))\n",
    "plt.xticks(np.arange(maxdeg) + 1,np.arange(maxdeg) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da0378",
   "metadata": {},
   "source": [
    "# c)\n",
    "\n",
    "To plot surface data retrieve the indices from the uniqueness of the xy-coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11140dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "x, y = np.meshgrid(np.arange(0,1,0.01),np.arange(0,1,0.01))\n",
    "xvec = np.array([x,y])\n",
    "X = make_design_matrix(xvec = xvec, p = 4)\n",
    "z = FrankeFunction(**{'x%i'%i: xvec[i].ravel() for i in range(len(xvec))})\n",
    "noise = np.random.normal(0,.1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "ztilde_train, ztilde_test, betahat, Xtrain, Xtest, ztrain,ztest = ols_fp_train_test_split(X = X, y = znoisy, \n",
    "                                                                                          **{\"test_size\" : 0.25, \n",
    "                                                                                             \"random_state\" : 42})\n",
    "xtrain = Xtrain[:,1]\n",
    "ytrain = Xtrain[:,2]\n",
    "shape_train = (x.shape[0], len(ytrain)//x.shape[0])\n",
    "xtest = Xtest[:,1]\n",
    "ytest = Xtest[:,2]\n",
    "shape_test = (x.shape[0], len(ytest)//x.shape[0])\n",
    "\n",
    "fig, axs = plt.subplots(2,2,subplot_kw={'projection':'3d'}, figsize=(10,10))\n",
    "axs[0,0].scatter(xtrain, ytrain, ztilde_train, c=ztilde_train, cmap=cm.viridis)\n",
    "#axs[0].scatter(xtrain, ytrain, ztrain, c=ztrain, cmap=cm.coolwarm)\n",
    "axs[0,1].scatter(xtrain, ytrain, np.mean(z)+ztrain-ztilde_train, c=np.mean(z)+ztrain-ztilde_train, cmap=cm.coolwarm)\n",
    "\n",
    "axs[1,0].scatter(xtest, ytest, ztilde_test, c=ztilde_test, cmap=cm.viridis)\n",
    "#axs[1].scatter(xtest, ytest, ztest, c=ztest, cmap=cm.coolwarm)\n",
    "axs[1,1].scatter(xtest, ytest, ztest + np.mean(z)-ztilde_test, c=np.mean(z)+ztest-ztilde_test, cmap=cm.coolwarm)\n",
    "[ax.view_init(15,45) for ax in axs.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2b2cb",
   "metadata": {},
   "source": [
    "# Bias-variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903555ae",
   "metadata": {},
   "source": [
    "### 1D test case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57eacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_exp(**kwargs):\n",
    "    x = kwargs['x0']\n",
    "    return np.exp(x -x**2) + 5*x**2 + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070eac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2022)\n",
    "import importlib; import project1; importlib.reload(project1)\n",
    "from project1.project1 import *\n",
    "model_complexity = 20\n",
    "mses_train, Rs_train = np.zeros((model_complexity)), np.zeros((model_complexity))\n",
    "mses_test, Rs_test = np.zeros((model_complexity)), np.zeros((model_complexity))\n",
    "x = np.arange(0,1,0.01)\n",
    "xvec = np.array([x])\n",
    "z = test_func_exp(**{'x%i'%i: xvec[i].ravel() for i in range(len(xvec))})\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "for p in range(1,model_complexity+1):\n",
    "    X = make_design_matrix(xvec = xvec, p = p)\n",
    "    \n",
    "    ztilde_train, ztilde_test, betahat, Xtrain, Xtest, ztrain,ztest = ols_fp_train_test_split(X = X, y = znoisy, \n",
    "                                                                                              **{\"test_size\" : 0.3, \n",
    "                                                                                                \"random_state\" : 42})\n",
    "    mses_train[p-1] = MSE(y = ztrain,ytilde = ztilde_train)\n",
    "    Rs_train[p-1] = Rscore(y = ztrain, ytilde = ztilde_train)\n",
    "    \n",
    "    mses_test[p-1] = MSE(y = ztest,ytilde = ztilde_test)\n",
    "    Rs_test[p-1] = Rscore(y = ztest, ytilde = ztilde_test)\n",
    "    \n",
    "ps = [i for i in range(1,model_complexity+1)]\n",
    "plt.plot(ps, np.log10(mses_train), label=\"MSE train\")\n",
    "plt.plot(ps, np.log10(mses_test), label=\"MSE test\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"log10 MSE\")\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.xticks(np.arange(model_complexity)[::2]+1,np.arange(20)[::2]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27ac75a",
   "metadata": {},
   "source": [
    "## 2D Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_2(**kwargs):\n",
    "    x = kwargs['x0']\n",
    "    y = kwargs['x1']\n",
    "    return np.exp(x -(x+y)**2) + 5*x**2 + x*y\n",
    "\n",
    "model_complexity = 10\n",
    "mses_train, Rs_train = np.zeros((model_complexity)), np.zeros((model_complexity))\n",
    "mses_test, Rs_test = np.zeros((model_complexity)), np.zeros((model_complexity))\n",
    "x, y = np.meshgrid(np.arange(0,1,0.05),np.arange(0,1,0.05))\n",
    "xvec = np.array([x,y])\n",
    "z = test_func_2(**{'x%i'%i: xvec[i].ravel() for i in range(len(xvec))})\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "for p in range(1,model_complexity+1):\n",
    "    X = make_design_matrix(xvec = xvec, p = p)\n",
    "    \n",
    "    ztilde_train, ztilde_test, betahat, Xtrain, Xtest, ztrain,ztest = ols_fp_train_test_split(X = X, y = znoisy, \n",
    "                                                                                              **{\"test_size\" : 0.3, \n",
    "                                                                                                \"random_state\" : 42})\n",
    "    mses_train[p-1] = MSE(y = ztrain,ytilde = ztilde_train)\n",
    "    Rs_train[p-1] = Rscore(y = ztrain, ytilde = ztilde_train)\n",
    "    \n",
    "    mses_test[p-1] = MSE(y = ztest,ytilde = ztilde_test)\n",
    "    Rs_test[p-1] = Rscore(y = ztest, ytilde = ztilde_test)\n",
    "# plot log10(MSE) wrt to the polynomial degree\n",
    "ps = [i for i in range(1,model_complexity+1)]\n",
    "plt.plot(ps, np.log10(mses_train), label=\"MSE train\")\n",
    "plt.plot(ps, np.log10(mses_test), label=\"MSE test\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"log10 MSE\")\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.xticks(np.arange(model_complexity)[::2]+1,np.arange(model_complexity)[::2]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71527e5",
   "metadata": {},
   "source": [
    "# Bias-variance trade-off Franke function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_complexity = 20\n",
    "x, y = np.meshgrid(np.arange(0,1,0.05),np.arange(0,1,0.05))\n",
    "xvec = np.array([x,y])\n",
    "mses_train, Rs_train, bias_train, variance_train = np.zeros((model_complexity)), np.zeros((model_complexity)),np.zeros((model_complexity)), np.zeros((model_complexity))\n",
    "mses_test, Rs_test, bias_test, variance_test = np.zeros((model_complexity)), np.zeros((model_complexity)),np.zeros((model_complexity)), np.zeros((model_complexity))\n",
    "\n",
    "z = FrankeFunction(**{'x%i'%i: xvec[i].ravel() for i in range(len(xvec))})\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "for p in range(1,model_complexity+1):\n",
    "    X = make_design_matrix(xvec = xvec, p = p)\n",
    "    ztilde_train, ztilde_test, betahat, Xtrain, Xtest, ztrain,ztest = ols_fp_train_test_split(X = X, y = znoisy, \n",
    "                                                                                              **{\"test_size\" : 0.3, \n",
    "                                                                                                \"random_state\" : 42})\n",
    "    \n",
    "    mses_train[p-1] = MSE(y = ztrain,ytilde = ztilde_train)\n",
    "    Rs_train[p-1] = Rscore(y = ztrain, ytilde = ztilde_train)\n",
    "    bias_train[p-1] = np.mean((ztrain-np.mean(ztilde_train))**2)\n",
    "    variance_train[p-1] = np.var(ztilde_train)\n",
    "    \n",
    "    mses_test[p-1] = MSE(y = ztest,ytilde = ztilde_test)\n",
    "    Rs_test[p-1] = Rscore(y = ztest, ytilde = ztilde_test)\n",
    "    bias_test[p-1] = np.mean((ztest-np.mean(ztilde_test))**2)\n",
    "    variance_test[p-1] = np.var(ztilde_test)\n",
    "\n",
    "# plot log10(MSE) wrt polynomial degrees\n",
    "ps = [i for i in range(1,model_complexity+1)]\n",
    "#plt.plot(ps, np.log10(mses_train), label=\"MSE train\")\n",
    "plt.plot(ps, np.log10(mses_test), label=\"MSE test\")\n",
    "#plt.plot(ps, np.log10(bias_train), label=\"Bias train\")\n",
    "plt.plot(ps, np.log10(bias_test), label=\"Bias test\")\n",
    "#plt.plot(ps, np.log10(variance_train), label=\"Var train\")\n",
    "plt.plot(ps, np.log10(variance_test), label=\"Var test\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"log10(MSE)\")\n",
    "plt.xlabel(\"Polynomial degree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd8af5",
   "metadata": {},
   "source": [
    "# Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "model_complexity = 20\n",
    "k = 100\n",
    "x, y = np.meshgrid(np.arange(0,1,0.05),np.arange(0,1,0.05))\n",
    "xvec = np.array([x,y])\n",
    "mses_train, Rs_train, bias_train, variance_train = np.zeros((model_complexity, k)), np.zeros((model_complexity,k)),np.zeros((model_complexity, k)), np.zeros((model_complexity,k))\n",
    "mses_test, Rs_test, bias_test, variance_test = np.zeros((model_complexity, k)), np.zeros((model_complexity, k)),np.zeros((model_complexity, k)), np.zeros((model_complexity, k))\n",
    "\n",
    "z = FrankeFunction(**{'x%i'%i: xvec[i].ravel() for i in range(len(xvec))})\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "\n",
    "for p in range(1,model_complexity+1):\n",
    "    print(p)\n",
    "    X = make_design_matrix(xvec = xvec, p = p)\n",
    "    Xtrain, Xtest, ztrain,ztest = train_test_split(X, znoisy,**{\"test_size\" : 0.25, \"random_state\" : 42})\n",
    "    for ik in range(k):\n",
    "        z_resampled = resample(data = ztrain)\n",
    "        X_resampled = resample(data = Xtrain)\n",
    "        ztilde_train, betahat = ols_fp_wo_split(X=Xtrain, y=ztrain)\n",
    "        ztilde_test = Xtest@betahat\n",
    "        mses_train[p-1, ik] = MSE(y = ztrain,ytilde = ztilde_train)\n",
    "        Rs_train[p-1, ik] = Rscore(y = ztrain, ytilde = ztilde_train)\n",
    "        bias_train[p-1, ik] = np.mean((ztrain-np.mean(ztilde_train))**2)\n",
    "        variance_train[p-1, ik] = np.var(ztilde_train)\n",
    "\n",
    "        mses_test[p-1, ik] = MSE(y = ztest,ytilde = ztilde_test)\n",
    "        Rs_test[p-1, ik] = Rscore(y = ztest, ytilde = ztilde_test)\n",
    "        bias_test[p-1, ik] = np.mean((ztest-np.mean(ztilde_test))**2)\n",
    "        variance_test[p-1, ik] = np.var(ztilde_test)\n",
    "\n",
    "# plot log10(MSE) wrt polynomial degrees\n",
    "ps = [i for i in range(1,model_complexity+1)]\n",
    "#plt.plot(ps, np.log10(np.mean(mses_train, axis = 1)), label=\"MSE train\")\n",
    "plt.plot(ps, np.log10(np.mean(mses_test, axis = 1)), label=\"MSE test\")\n",
    "#plt.plot(ps, np.log10(np.mean(bias_train, axis = 1)), label=\"Bias train\")\n",
    "plt.plot(ps, np.log10(np.mean(bias_test, axis = 1)), label=\"Bias test\")\n",
    "#plt.plot(ps, np.log10(np.mean(variance_train, axis = 1)), label=\"Var train\")\n",
    "plt.plot(ps, np.log10(np.mean(variance_test, axis = 1)), label=\"Var test\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"log10(MSE)\")\n",
    "plt.xlabel(\"Polynomial degree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6246ca",
   "metadata": {},
   "source": [
    "# d) cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b050f7",
   "metadata": {},
   "source": [
    "### Testing the standardscaler from SL and compare it to own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb4854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "N = 10; p=2\n",
    "x, y = np.meshgrid(np.linspace(0,1,N),np.linspace(0,1,N))\n",
    "X = create_X(x = x, y=y, n=p)\n",
    "print(X)\n",
    "scaler = StandardScaler(with_std=True)\n",
    "scaler.fit(X)\n",
    "Xscaled_SL = scaler.transform(X)\n",
    "Xscaled = scale_center_X(X=X)\n",
    "print(Xscaled_SL, Xscaled)\n",
    "Xscaled_SL == Xscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296fd40",
   "metadata": {},
   "source": [
    "## Franke function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9969221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "model_complexity = 20\n",
    "x, y = np.meshgrid(np.arange(0,1,0.05),np.arange(0,1,0.05))\n",
    "xvec = np.array([x,y])\n",
    "z = FrankeFunction(**{'x%i'%i: xvec[i].ravel() for i in range(len(xvec))})\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "k = 5\n",
    "mses_train, var_train, bias_train, variance_train = np.zeros((model_complexity, k)), np.zeros((model_complexity,k)),np.zeros((model_complexity, k)), np.zeros((model_complexity,k))\n",
    "mses_test, var_test, bias_test, variance_test = np.zeros((model_complexity, k)), np.zeros((model_complexity, k)),np.zeros((model_complexity, k)), np.zeros((model_complexity, k))\n",
    "for p in range(1,model_complexity+1):\n",
    "    mses_train[p-1], mses_test[p-1], bias_train[p-1], bias_test[p-1], var_train[p-1], var_test[p-1] = cross_validation(data = znoisy, xvec = xvec, k = k, p = p, method = \"ols\")\n",
    "# plot log10(MSE) wrt polynomial degree\n",
    "ps = [i for i in range(1,model_complexity+1)]\n",
    "#plt.plot(ps, np.log10(np.mean(mses_train, axis = 1)), label=\"MSE train\")\n",
    "plt.plot(ps, np.log10(np.mean(mses_test, axis = 1)), label=\"MSE test\")\n",
    "#plt.plot(ps, np.log10(np.mean(bias_train, axis = 1)), label=\"Bias train\")\n",
    "plt.plot(ps, np.log10(np.mean(bias_test, axis = 1)), label=\"Bias test\")\n",
    "#plt.plot(ps, np.log10(np.mean(var_train, axis = 1)), label=\"Var train\")\n",
    "plt.plot(ps, np.log10(np.mean(var_test, axis = 1)), label=\"Var test\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"log10(MSE)\")\n",
    "plt.xlabel(\"Polynomial degree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8f35b",
   "metadata": {},
   "source": [
    "### Ridge comparison $\\lambda = 0$\n",
    "- should give similar features as the OLS\n",
    "    - but bias is tending to increase with the polynomial degree..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "model_complexity = 20\n",
    "x, y = np.meshgrid(np.arange(0,1,0.05),np.arange(0,1,0.05))\n",
    "xvec = np.array([x,y])\n",
    "z = FrankeFunction(**{'x%i'%i: xvec[i] for i in range(len(xvec))})\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "k = 5\n",
    "mses_train, var_train, bias_train, variance_train = np.zeros((model_complexity, k)), np.zeros((model_complexity,k)),np.zeros((model_complexity, k)), np.zeros((model_complexity,k))\n",
    "mses_test, var_test, bias_test, variance_test = np.zeros((model_complexity, k)), np.zeros((model_complexity, k)),np.zeros((model_complexity, k)), np.zeros((model_complexity, k))\n",
    "for p in range(1,model_complexity+1):\n",
    "    mses_train[p-1], mses_test[p-1], bias_train[p-1], bias_test[p-1], var_train[p-1], var_test[p-1] = cross_validation(data = znoisy, xvec = xvec, k = k, p = p, method = \"ridge\", lmbda=0, scale_centering=True)\n",
    "# plot log10(MSE) wrt polynomial degree\n",
    "ps = [i for i in range(1,model_complexity+1)]\n",
    "#plt.plot(ps, np.log10(np.mean(mses_train, axis = 1)), label=\"MSE train\")\n",
    "plt.plot(ps, np.log10(np.mean(mses_test, axis = 1)), label=\"MSE test\")\n",
    "#plt.plot(ps, np.log10(np.mean(bias_train, axis = 1)), label=\"Bias train\")\n",
    "plt.plot(ps, np.log10(np.mean(bias_test, axis = 1)), label=\"Bias test\")\n",
    "#plt.plot(ps, np.log10(np.mean(var_train, axis = 1)), label=\"Var train\")\n",
    "plt.plot(ps, np.log10(np.mean(var_test, axis = 1)), label=\"Var test\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"log10(MSE)\")\n",
    "plt.xlabel(\"Polynomial degree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1747c492",
   "metadata": {},
   "source": [
    "# e) Ridge with CV and bootstrap\n",
    "- Bias-variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271afb8a",
   "metadata": {},
   "source": [
    "### Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x, y = np.meshgrid(np.arange(0,1,0.05),np.arange(0,1,0.05))\n",
    "z = FrankeFunction(**{'x%i'%i: xvec[i] for i in range(len(xvec))})\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "model_complexity = 20\n",
    "k = 10\n",
    "kfold = KFold(n_splits = k)\n",
    "lambdas = np.array([0]+list(np.logspace(-5,2,10)))\n",
    "mses_train_ridge_skl, bias_train_ridge_skl, var_train_ridge_skl = np.zeros((model_complexity, len(lambdas), k)),np.zeros((model_complexity, len(lambdas), k)),np.zeros((model_complexity, len(lambdas), k))\n",
    "mses_test_ridge_skl, bias_test_ridge_skl, var_test_ridge_skl = np.zeros((model_complexity, len(lambdas), k)),np.zeros((model_complexity, len(lambdas), k)),np.zeros((model_complexity, len(lambdas), k))\n",
    "\n",
    "for p in range(1, model_complexity+1):\n",
    "    print(p)\n",
    "    poly = PolynomialFeatures(degree = p)\n",
    "\n",
    "    # Decide degree on polynomial to fit\n",
    "    for i in range(len(lambdas)):\n",
    "        ridge_model = linear_model.Ridge(alpha = lambdas[i])\n",
    "        j = 0\n",
    "        for train_inds, test_inds in kfold.split(x.ravel()):\n",
    "            xtrain, ytrain, ztrain = x.ravel()[train_inds],y.ravel()[train_inds], znoisy.ravel()[train_inds]\n",
    "            xtest, ytest, ztest = x.ravel()[test_inds],y.ravel()[test_inds], znoisy.ravel()[test_inds]\n",
    "            #Xtrain = make_design_matrix(xvec = np.array([xtrain, ytrain]), p = p)[:,1:]\n",
    "            #Xtrain = create_X(x = xtrain, y= ytrain, n = p)[:,1:]\n",
    "            Xtrain = poly.fit_transform(np.hstack([xtrain[:, np.newaxis],ytrain[:, np.newaxis]]))\n",
    "            #Xtest = make_design_matrix(xvec = np.array([xtest, ytest]), p = p)[:,1:]\n",
    "            #Xtest = create_X(x = xtest, y= ytest, n = p)[:,1:]\n",
    "            Xtest = poly.fit_transform(np.hstack([xtest[:, np.newaxis],ytest[:, np.newaxis]]))\n",
    "            \n",
    "            ridge_model.fit(Xtrain, ztrain)\n",
    "            ztilde_train= Xtrain@ridge_model.coef_\n",
    "            ztilde_test = Xtest@ridge_model.coef_\n",
    "            mses_train_ridge_skl[p-1,i, j] = MSE(y = ztrain, ytilde = ztilde_train)\n",
    "            mses_test_ridge_skl[p-1,i, j] = MSE(y = ztest, ytilde = ztilde_test)\n",
    "            \n",
    "            bias_train_ridge_skl[p-1,i, j] = bias(y = ztrain, ytilde = ztilde_train)\n",
    "            bias_test_ridge_skl[p-1,i, j] = bias(y = ztest, ytilde = ztilde_test)\n",
    "            \n",
    "            var_train_ridge_skl[p-1,i, j] = np.var(ztilde_train)\n",
    "            var_test_ridge_skl[p-1,i, j] = np.var(ztilde_test)\n",
    "            \n",
    "            j+=1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60563f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot log10(MSE) wrt polynomial degree\n",
    "ps = [i for i in range(1,model_complexity+1)]\n",
    "for i in range(len(lambdas)):\n",
    "    fig, ax = plt.subplots()\n",
    "    if lambdas[i] == 0:\n",
    "        ax.set_title(\"$\\\\lambda = 0$\")\n",
    "    else:\n",
    "        ax.set_title(\"$\\\\lambda = 10^{%.2f}$\"%np.log10(lambdas[i]))\n",
    "    ax.plot(ps, np.log10(np.mean(mses_test_ridge_skl[:,i], axis=1)), label=\"MSE test\")\n",
    "    ax.plot(ps, np.log10(np.mean(bias_test_ridge_skl[:,i], axis=1)), label=\"Bias test\")\n",
    "    ax.plot(ps, np.log10(np.mean(var_test_ridge_skl[:,i], axis=1)), label=\"Var test\")\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(\"log10(MSE)\")\n",
    "    ax.set_xlabel(\"Polynomial degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fe28d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "carr = np.arange(mses_train_ridge_skl.shape[0])/mses_train_ridge_skl.shape[0]\n",
    "cs_train = plt.cm.Blues(carr)\n",
    "cs_test = plt.cm.Oranges(carr)\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "for ip in range(mses_train.shape[0]):\n",
    "    ax.plot(lambdas, mses_train_ridge_skl[ip], color = cs_train[ip], label=\"Train p = %i\"%(ip+1))\n",
    "    ax.plot(lambdas, mses_test_ridge_skl[ip], color = cs_test[ip], label=\"Test p = %i\"%(ip+1))\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(0.6,10)\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_xlabel(\"log10$\\\\lambda$\")\n",
    "ax.legend(bbox_to_anchor=(1.1,.9,0,.1), ncol=2)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72a4a0",
   "metadata": {},
   "source": [
    "## CV\n",
    "- check: for $\\lambda = 0$ do I get the same as the OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010bf990",
   "metadata": {},
   "source": [
    "## Simple test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25f07fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "\n",
    "# A seed just to ensure that the random numbers are the same for every run.\n",
    "# Useful for eventual debugging.\n",
    "np.random.seed(3155)\n",
    "\n",
    "# Generate the data.\n",
    "nsamples = 1000\n",
    "x = np.random.randn(nsamples)\n",
    "y = 3*x**2 + np.random.randn(nsamples)\n",
    "p = 6\n",
    "k = 5\n",
    "# Decide which values of lambda to use\n",
    "nlambdas = 500\n",
    "lambdas = np.logspace(-3, 5, nlambdas)\n",
    "mses_train, var_train, bias_train = np.zeros((len(lambdas), k)), np.zeros((len(lambdas), k)),np.zeros((len(lambdas), k))\n",
    "mses_test, var_test, bias_test = np.zeros((len(lambdas), k)), np.zeros((len(lambdas), k)),np.zeros((len(lambdas), k))\n",
    "for i in range(len(lambdas)):\n",
    "    mses_train[i], mses_test[i], bias_train[i], bias_test[i], var_train[i], var_test[i] = cross_validation(data = y, xvec = np.array([x]), k = k, p = p, method = \"ridge\", lmbda = lambdas[i], scale_centering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fde326c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold\n",
    "# Initialize a KFold instance\n",
    "k = 5\n",
    "kfold = KFold(n_splits = k)\n",
    "poly = PolynomialFeatures(degree = 6)\n",
    "estimated_mse_sklearn = np.zeros(nlambdas)\n",
    "i = 0\n",
    "for lmb in lambdas:\n",
    "    ridge = Ridge(alpha = lmb)\n",
    "\n",
    "    X_SL = poly.fit_transform(x[:, np.newaxis])\n",
    "    X_SL_scaled = scale_center_X(X = X_SL[:,1:])\n",
    "    estimated_mse_folds = cross_val_score(ridge, X_SL_scaled, y[:, np.newaxis], scoring='neg_mean_squared_error', cv=kfold)\n",
    "\n",
    "    # cross_val_score return an array containing the estimated negative mse for every fold.\n",
    "    # we have to the the mean of every array in order to get an estimate of the mse of the model\n",
    "    estimated_mse_sklearn[i] = np.mean(-estimated_mse_folds)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a26b764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb0e280a250>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log10(lambdas), np.log10(np.mean(mses_test, axis=1)), label=\"MSE test\")\n",
    "#plt.plot(np.log10(lambdas), np.log10(np.mean(mses_train, axis=1)))\n",
    "\n",
    "plt.plot(np.log10(lambdas), np.log10(estimated_mse_sklearn), label = 'cross_val_score')\n",
    "#plt.plot(np.log10(lambdas), np.log10(np.mean(mses_test, axis=1))-np.log10(estimated_mse_sklearn), label = 'diff')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e01c81",
   "metadata": {},
   "source": [
    "## Franke function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f20332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "x, y = np.meshgrid(np.arange(0,1,0.01),np.arange(0,1,0.01))\n",
    "xvec = np.array([x,y])\n",
    "z = FrankeFunction(**{'x%i'%i: xvec[i] for i in range(len(xvec))})\n",
    "noise = np.random.normal(0,1,size=z.shape)\n",
    "znoisy = z + noise\n",
    "model_complexity = 20\n",
    "k = 5\n",
    "lambdas = np.array([0]+list(np.logspace(-5,5,10)))\n",
    "mses_train, bias_train, var_train = np.zeros((model_complexity, len(lambdas), k)),np.zeros((model_complexity, len(lambdas), k)),np.zeros((model_complexity, len(lambdas), k))\n",
    "mses_test, bias_test, var_test = np.zeros((model_complexity, len(lambdas), k)),np.zeros((model_complexity, len(lambdas), k)),np.zeros((model_complexity, len(lambdas), k))\n",
    "for p in range(1, model_complexity+1):\n",
    "    print(p)\n",
    "    X = make_design_matrix(xvec = xvec, p = p)\n",
    "    for i in range(len(lambdas)):\n",
    "        #print(lambdas[i])\n",
    "        mses_train[p-1, i], mses_test[p-1, i], bias_train[p-1, i], bias_test[p-1, i], var_train[p-1, i], var_test[p-1, i] = cross_validation(data = znoisy, xvec = xvec, k = k, p = p, method = \"ridge\", lmbda = lambdas[i], scale_centering=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,len(lambdas), figsize=(len(lambdas)*5,5), sharey=True)\n",
    "\n",
    "ps = np.arange(model_complexity)+1\n",
    "\n",
    "for i in range(1,len(lambdas)):\n",
    "    if lambdas[i] == 0:\n",
    "        axs[i].set_title(\"$\\\\lambda = 0$\")\n",
    "    else:    \n",
    "        axs[i].set_title(\"$\\\\lambda = 10^{%.3f}$\"%np.log10(lambdas[i]))\n",
    "    axs[i].plot(ps, np.log10(np.mean(mses_train[:,i],axis=1)),  label=\"MSE train\", color = 'b')\n",
    "    axs[i].plot(ps, np.log10(np.mean(mses_test[:,i], axis=1)),  label=\"MSE test\",  color = 'b', ls='--')\n",
    "    #axs[i].plot(ps, np.log10(np.mean(bias_train[:,i],axis=1)),  label=\"Bias train\",color = 'C1')\n",
    "    #axs[i].plot(ps, np.log10(np.mean(bias_test[:,i], axis=1)),  label=\"Bias test\", color = 'C1', ls='--')\n",
    "    #axs[i].plot(ps, np.log10(np.mean(var_train[:,i], axis=1)),  label=\"Var train\", color = 'C2')\n",
    "    #axs[i].plot(ps, np.log10(np.mean(var_test[:,i],  axis=1)),  label=\"Var test\",  color = 'C2', ls='--')\n",
    "[ax.set_ylabel(\"log10(MSE)\") for ax in axs]\n",
    "[ax.set_xlabel(\"Polynomial degree\") for ax in axs]\n",
    "[ax.legend() for ax in axs]\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2339857",
   "metadata": {},
   "outputs": [],
   "source": [
    "carr = np.arange(mses_train.shape[0])/mses_train.shape[0]\n",
    "cs_train = plt.cm.Blues(carr)\n",
    "cs_test = plt.cm.Oranges(carr)\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "for ip in range(mses_train.shape[0]):\n",
    "    ax.plot(lambdas, np.mean(mses_train[ip], axis=1), color = cs_train[ip], label=\"Train p = %i\"%(ip+1))\n",
    "    ax.plot(lambdas, np.mean(mses_test[ip], axis=1), color = cs_test[ip], label=\"Test p = %i\"%(ip+1))\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(1,1.1)\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_xlabel(\"$\\\\lambda$\")\n",
    "ax.legend(bbox_to_anchor=(1.1,.9,0,.1), ncol=2)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d36928",
   "metadata": {},
   "source": [
    "## with bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "model_complexity = 20\n",
    "k = 100\n",
    "x, y = np.meshgrid(np.arange(0,1,0.01),np.arange(0,1,0.01))\n",
    "xvec = np.array([x,y])\n",
    "lambdas = np.logspace(-5,5, 10)\n",
    "\n",
    "mses_train, var_train, bias_train = np.zeros((model_complexity,len(lambdas), k)), np.zeros((model_complexity,len(lambdas),k)),np.zeros((model_complexity, len(lambdas),k)) \n",
    "mses_test, var_test, bias_test = np.zeros((model_complexity,len(lambdas), k)), np.zeros((model_complexity, len(lambdas), k)),np.zeros((model_complexity,len(lambdas), k))\n",
    "for p in range(1,model_complexity+1):\n",
    "    print(p)\n",
    "    X = make_design_matrix(xvec = xvec, p = p)\n",
    "    z = FrankeFunction(**{'x%i'%i: xvec[i].ravel() for i in range(len(xvec))})\n",
    "    noise = np.random.normal(0,1,size=z.shape)\n",
    "    znoisy = z + noise\n",
    "    Xtrain, Xtest, ztrain, ztest = train_test_split(X, znoisy)\n",
    "    for i in range(len(lambdas)):\n",
    "        for ik in range(k):\n",
    "            z_resampled = resample(data = ztrain)\n",
    "            X_resampled = resample(data = Xtrain)\n",
    "            ztilde_train, betahat = ridge_fp_wo_split(X=X_resampled, y=z_resampled-np.mean(z_resampled), lmbda = lambdas[i])\n",
    "            ztilde_train += np.mean(z_resampled)\n",
    "            ztilde_test = Xtest@betahat + np.mean(z_resampled)\n",
    "            mses_train[p-1, i, ik] = MSE(y = z_resampled ,ytilde = ztilde_train)\n",
    "            bias_train[p-1, i, ik] = np.mean((z_resampled-np.mean(ztilde_train))**2)\n",
    "            var_train[p-1, i, ik] = np.var(ztilde_train)\n",
    "\n",
    "            mses_test[p-1, i, ik] = MSE(y = ztest,ytilde = ztilde_test)\n",
    "            bias_test[p-1, i, ik] = np.mean((ztest-np.mean(ztilde_test))**2)\n",
    "            var_test[p-1, i, ik] = np.var(ztilde_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfb396",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,len(lambdas), figsize=(len(lambdas)*5,5), sharey=True)\n",
    "\n",
    "ps = np.arange(model_complexity)+1\n",
    "\n",
    "for i in range(1,len(lambdas)):\n",
    "    if lambdas[i] == 0:\n",
    "        axs[i].set_title(\"$\\\\lambda = 0$\")\n",
    "    else:    \n",
    "        axs[i].set_title(\"$\\\\lambda = 10^{%.3f}$\"%np.log10(lambdas[i]))\n",
    "    axs[i].plot(ps, np.log10(np.mean(mses_train[:,i],axis=1)),  label=\"MSE train\", color = 'b')\n",
    "    axs[i].plot(ps, np.log10(np.mean(mses_test[:,i], axis=1)),  label=\"MSE test\",  color = 'b', ls='--')\n",
    "    #axs[i].plot(ps, np.log10(np.mean(bias_train[:,i],axis=1)),  label=\"Bias train\",color = 'C1')\n",
    "    axs[i].plot(ps, np.log10(np.mean(bias_test[:,i], axis=1)),  label=\"Bias test\", color = 'C1', ls='--')\n",
    "    #axs[i].plot(ps, np.log10(np.mean(var_train[:,i], axis=1)),  label=\"Var train\", color = 'C2')\n",
    "    axs[i].plot(ps, np.log10(np.mean(var_test[:,i],  axis=1)),  label=\"Var test\",  color = 'C2', ls='--')\n",
    "[ax.set_ylabel(\"log10(MSE)\") for ax in axs]\n",
    "[ax.set_xlabel(\"Polynomial degree\") for ax in axs]\n",
    "[ax.legend() for ax in axs]\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,model_complexity,figsize=(model_complexity*5,5))\n",
    "for p in range(model_complexity):\n",
    "    axs[p].set_title(\"$p = %i$\"%(p+1))\n",
    "    axs[p].plot(lambdas, np.log10(np.mean(mses_train[p], axis=1)), label=\"MSE Train\")\n",
    "    axs[p].plot(lambdas, np.log10(np.mean(mses_test[p], axis=1)), label=\"MSE Test\") \n",
    "    #axs[p].plot(lambdas, np.log10(np.mean(bias_train[p], axis=1)), label=\"Bias Train\")\n",
    "    #axs[p].plot(lambdas, np.log10(np.mean(bias_test[p], axis=1)), label=\"Bias Test\") \n",
    "    #axs[p].plot(lambdas, np.log10(np.mean(var_train[p], axis=1)), label=\"$\\\\sigma^2$ Train\")\n",
    "    #axs[p].plot(lambdas, np.log10(np.mean(var_test[p], axis=1)), label=\"$\\\\sigma^2$ Test\") \n",
    "[ax.set_xlabel('log10($\\\\lambda$)') for ax in axs]\n",
    "[ax.set_xscale('log') for ax in axs]\n",
    "[ax.legend() for ax in axs]\n",
    "[ax.set_ylabel(\"log10(MSE)\") for ax in axs]\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,len(lambdas), figsize=(len(lambdas)*5,5))\n",
    "\n",
    "ps = np.arange(model_complexity)+1\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    if lambdas[i] == 0:\n",
    "        axs[i].set_title(\"$\\\\lambda = 0$\")\n",
    "    else:    \n",
    "        axs[i].set_title(\"$\\\\lambda = 10^{%.3f}$\"%np.log10(lambdas[i]))\n",
    "    #axs[i].plot(ps, np.log10(np.mean(mses_train[:,i],axis=1)),  label=\"MSE train\", color = 'b')\n",
    "    axs[i].plot(ps, np.log10(np.mean(mses_test[:,i], axis=1)),  label=\"MSE test\",  color = 'b', ls='--')\n",
    "    #axs[i].plot(ps, np.log10(np.mean(bias_train[:,i],axis=1)),  label=\"Bias train\",color = 'C1')\n",
    "    axs[i].plot(ps, np.log10(np.mean(bias_test[:,i], axis=1)),  label=\"Bias test\", color = 'C1', ls='--')\n",
    "    #axs[i].plot(ps, np.log10(np.mean(var_train[:,i], axis=1)),  label=\"Var train\", color = 'C2')\n",
    "    axs[i].plot(ps, np.log10(np.mean(var_test[:,i],  axis=1)),  label=\"Var test\",  color = 'C2', ls='--')\n",
    "[ax.set_ylabel(\"log10(MSE)\") for ax in axs]\n",
    "[ax.set_xlabel(\"Polynomial degree\") for ax in axs]\n",
    "[ax.legend() for ax in axs]\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b74bb7",
   "metadata": {},
   "source": [
    "# f) Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952aaf92",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5acc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled = resample(data = X)\n",
    "X_resampled.reshape(X.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "model_complexity = 10\n",
    "k = 1000\n",
    "x, y = np.meshgrid(np.arange(0,1,0.01),np.arange(0,1,0.01))\n",
    "xvec = np.array([x,y])\n",
    "lambdas = np.logspace(-6,4, 100)\n",
    "\n",
    "mses_train, var_train, bias_train = np.zeros((model_complexity,len(lambdas), k)), np.zeros((model_complexity,len(lambdas),k)),np.zeros((model_complexity, len(lambdas),k)) \n",
    "mses_test, var_test, bias_test = np.zeros((model_complexity,len(lambdas), k)), np.zeros((model_complexity, len(lambdas), k)),np.zeros((model_complexity,len(lambdas), k))\n",
    "for p in range(1,model_complexity+1):\n",
    "    print(p)\n",
    "    X = make_design_matrix(xvec = xvec, p = p)\n",
    "    X = X[:,1:]\n",
    "    z = FrankeFunction(**{'x%i'%i: xvec[i].ravel() for i in range(len(xvec))})\n",
    "    noise = np.random.normal(0,1,size=z.shape)\n",
    "    znoisy = z + noise\n",
    "    Xtrain, Xtest, ztrain, ztest = train_test_split(X, znoisy)\n",
    "    for i in range(len(lambdas)):\n",
    "        for ik in range(k):\n",
    "            z_resampled = resample(data = ztrain)\n",
    "            X_resampled = resample(data = Xtrain)\n",
    "            \n",
    "            ztilde_train, betahat = lasso_fp_wo_split(X=X_resampled, y=z_resampled-np.mean(z_resampled), lmbda = lambdas[i])\n",
    "            ztilde_train += np.mean(z_resampled)\n",
    "            ztilde_test = Xtest@betahat + np.mean(z_resampled)\n",
    "            mses_train[p-1, i, ik] = MSE(y = ztrain ,ytilde = ztilde_train)\n",
    "            bias_train[p-1, i, ik] = np.mean((ztrain-np.mean(ztilde_train))**2)\n",
    "            var_train[p-1, i, ik] = np.var(ztilde_train)\n",
    "\n",
    "            mses_test[p-1, i, ik] = MSE(y = ztest,ytilde = ztilde_test)\n",
    "            bias_test[p-1, i, ik] = np.mean((ztest-np.mean(ztilde_test))**2)\n",
    "            z_resampled = resample(data = ztrain)\n",
    "            var_test[p-1, i, ik] = np.var(ztilde_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,model_complexity,figsize=(model_complexity*5,5))\n",
    "for p in range(model_complexity):\n",
    "    axs[p].set_title(\"$p = %i$\"%(p+1))\n",
    "    axs[p].plot(lambdas, np.log10(np.mean(mses_train[p], axis=1)), label=\"MSE Train\")\n",
    "    axs[p].plot(lambdas, np.log10(np.mean(mses_test[p], axis=1)), label=\"MSE Test\") \n",
    "    #axs[p].plot(lambdas, np.log10(np.mean(bias_train[p], axis=1)), label=\"Bias Train\")\n",
    "    #axs[p].plot(lambdas, np.log10(np.mean(bias_test[p], axis=1)), label=\"Bias Test\") \n",
    "    #axs[p].plot(lambdas, np.log10(np.mean(var_train[p], axis=1)), label=\"$\\\\sigma^2$ Train\")\n",
    "    #axs[p].plot(lambdas, np.log10(np.mean(var_test[p], axis=1)), label=\"$\\\\sigma^2$ Test\") \n",
    "[ax.set_xscale('log') for ax in axs]\n",
    "[ax.legend() for ax in axs]\n",
    "[ax.set_ylabel(\"log10(MSE)\") for ax in axs]\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782e76d",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828cfa27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import project1.project1; import importlib; importlib.reload(project1.project1); from project1.project1 import *\n",
    "model_complexity = 5\n",
    "k=5\n",
    "x, y = np.meshgrid(np.arange(0,1,0.01),np.arange(0,1,0.01))\n",
    "xvec = np.array([x,y])\n",
    "lambdas = np.logspace(-4,3, 100)\n",
    "\n",
    "mses_train, var_train, bias_train, variance_train = np.zeros((model_complexity,len(lambdas), k)), np.zeros((model_complexity,len(lambdas),k)),np.zeros((model_complexity, len(lambdas),k)), np.zeros((model_complexity,len(lambdas),k))\n",
    "mses_test, var_test, bias_test, variance_test = np.zeros((model_complexity,len(lambdas), k)), np.zeros((model_complexity, len(lambdas), k)),np.zeros((model_complexity,len(lambdas), k)), np.zeros((model_complexity, len(lambdas),k))\n",
    "\n",
    "for p in range(1, model_complexity+1):\n",
    "    print(p)\n",
    "    #X = make_design_matrix(xvec = xvec, p = p)\n",
    "    X = create_X(x = x, y=y, n = p)\n",
    "    \n",
    "    z = FrankeFunction(**{'x%i'%i: xvec[i].ravel() for i in range(len(xvec))})\n",
    "    noise = np.random.normal(0,1,size=z.shape)\n",
    "    znoisy = z + noise\n",
    "    for i in range(len(lambdas)):\n",
    "        mses_train[p-1,i], mses_test[p-1,i], bias_train[p-1,i], bias_test[p-1,i], var_train[p-1,i], var_test[p-1,i] = cross_validation(data = znoisy, xvec = xvec, k = k, p = p, method = \"lasso\", lmbda = lambdas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,model_complexity,figsize=(model_complexity*5,5), sharey=True)\n",
    "for p in range(model_complexity):\n",
    "    axs[p].set_title(\"$p=%i$\"%(p+1))\n",
    "    axs[p].plot(lambdas, np.log10(np.mean(mses_train[p], axis=1)), label=\"Train\")\n",
    "    axs[p].plot(lambdas, np.log10(np.mean(mses_test[p], axis=1)), label=\"Test\") \n",
    "[ax.set_xscale('log') for ax in axs]\n",
    "[ax.legend() for ax in axs]\n",
    "axs[0].set_ylabel(\"log10(MSE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f27e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,len(lambdas)//10, figsize=(len(lambdas)//10*5,5))\n",
    "ps = np.arange(model_complexity)+1\n",
    "\n",
    "for i in range(0,len(lambdas),10):\n",
    "    axs[i//10].set_title(\"$\\\\lambda = 10^{%.3f}$\"%np.log10(lambdas[i]))\n",
    "    axs[i//10].plot(ps, np.log10(np.mean(mses_train[:,i], axis=1)), label=\"Train\", color='b')\n",
    "    axs[i//10].plot(ps, np.log10(np.mean(mses_test[:,i], axis=1)), label=\"Test\", color = 'C1')\n",
    "[ax.set_ylabel(\"log10(MSE)\") for ax in axs]\n",
    "[ax.set_xlabel(\"Polynomial degree\") for ax in axs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ab50e",
   "metadata": {},
   "source": [
    "# Real data \n",
    "# g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa50964",
   "metadata": {},
   "source": [
    "### plot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7af56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "datadir = \"~/cs/ada_ml/ada_ml_project1/datafiles\"\n",
    "terrain1 = np.array(imread(datadir + \"/SRTM_data_Norway_1.tif\"))\n",
    "terrain2 = np.array(imread(datadir + \"/SRTM_data_Norway_2.tif\"))\n",
    "fig, axs = plt.subplots(1,2, figsize = (10,5))\n",
    "fig_3d, axs_3d = plt.subplots(1,2, subplot_kw={'projection':'3d'}, figsize = (10,5))\n",
    "im1 = axs[0].imshow(terrain1, cmap = plt.cm.terrain)\n",
    "im2 = axs[1].imshow(terrain2, cmap = plt.cm.terrain)\n",
    "cbar1 = fig.colorbar(im1, ax = axs[0])\n",
    "cbar2 = fig.colorbar(im2, ax = axs[1])\n",
    "x1, y1 = np.meshgrid(np.linspace(0,1, terrain1.shape[1]), np.linspace(0,1, terrain1.shape[0]) )\n",
    "x2, y2 = np.meshgrid(np.linspace(0,1, terrain2.shape[1]), np.linspace(0,1, terrain2.shape[0]) )\n",
    "axs_3d[0].plot_surface(x1, y1, terrain1/np.sqrt(np.sum(terrain1**2)), cmap =plt.cm.terrain)\n",
    "axs_3d[1].plot_surface(x2, y2, terrain2/np.sqrt(np.sum(terrain2**2)), cmap =plt.cm.terrain)\n",
    "[ax.view_init(20,45) for ax in axs_3d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab19b9d",
   "metadata": {},
   "source": [
    "## Test OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a14836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "p = 10\n",
    "xvec = np.array([x1,y1])\n",
    "\n",
    "#X = make_design_matrix(xvec = xvec, p = p)\n",
    "X = create_X(x = x1, y=y1, n = p)\n",
    "Xtrain, Xtest, ztrain, ztest = train_test_split(X, terrain1.ravel(), **{'random_state' : 42, 'test_size' : 0.3})\n",
    "ztilde_train, betahat = ols_fp_wo_split(X=Xtrain, y=ztrain-np.mean(ztrain))\n",
    "ztilde_train += np.mean(ztrain)\n",
    "ztilde_test = Xtest@betahat + np.mean(ztrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9245782",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_3d, axs_3d = plt.subplots(1,2, subplot_kw={'projection':'3d'}, figsize = (10,5))\n",
    "axs_3d[0].scatter(Xtrain[:,1][::100], \n",
    "                  Xtrain[:,2][::100], \n",
    "                  ztilde_train[::100], \n",
    "                  c=ztilde_train[::100], cmap = plt.cm.terrain)\n",
    "axs_3d[1].scatter(Xtest[:,1][::100], \n",
    "                  Xtest[:,2][::100], \n",
    "                  ztilde_test[::100], \n",
    "                  c = ztilde_test[::100], cmap = plt.cm.terrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0496c50",
   "metadata": {},
   "source": [
    "## Bias-variance trade-off analysis\n",
    "1. OLS\n",
    "2. Ridge\n",
    "3. Lasso\n",
    "\n",
    "with bootstrap and cross-validation in order to make a full comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d7d3fd",
   "metadata": {},
   "source": [
    "### OLS bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3421e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "model_complexity = 10\n",
    "k = int(10)\n",
    "xvec = np.array([x1,y1])\n",
    "\n",
    "mses_train, var_train, bias_train = np.zeros((model_complexity, k)), np.zeros((model_complexity,k)),np.zeros((model_complexity,k)) \n",
    "mses_test, var_test, bias_test = np.zeros((model_complexity, k)), np.zeros((model_complexity, k)),np.zeros((model_complexity, k))\n",
    "for p in range(1,model_complexity+1):\n",
    "    print(p)\n",
    "    #X = make_design_matrix(xvec = xvec, p = p)\n",
    "    X = create_X(x = x1, y=y1, n = p)\n",
    "    Xtrain, Xtest, ztrain, ztest = train_test_split(X, terrain1.ravel())\n",
    "    for ik in range(k):\n",
    "        z_resampled = resample(data =ztrain)\n",
    "        ztilde_train, betahat = ols_fp_wo_split(X=Xtrain, y=z_resampled-np.mean(z_resampled))\n",
    "        ztilde_train += np.mean(z_resampled)\n",
    "        ztilde_test = Xtest@betahat + np.mean(z_resampled)\n",
    "        mses_train[p-1, ik] = MSE(y = ztrain ,ytilde = ztilde_train)\n",
    "        bias_train[p-1, ik] = np.mean((ztrain-np.mean(ztilde_train))**2)\n",
    "        var_train[p-1, ik] = np.var(ztilde_train)\n",
    "\n",
    "        mses_test[p-1,ik] = MSE(y = ztest ,ytilde = ztilde_test)\n",
    "        bias_test[p-1,ik] = np.mean((ztest-np.mean(ztilde_test))**2)\n",
    "        var_test[p-1, ik] = np.var(ztilde_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ps = np.arange(model_complexity)+1\n",
    "ax.set_title(\"$p = %i$\"%(model_complexity))\n",
    "ax.errorbar(ps, np.log10(np.mean(mses_train, axis=1)),yerr=np.log10(np.std(mses_train, axis=1)), label=\"MSE Train\")\n",
    "ax.errorbar(ps, np.log10(np.mean(mses_test, axis=1)), yerr=np.log10(np.std(mses_test, axis=1)),  label=\"MSE Test\" ) \n",
    "ax.errorbar(ps, np.log10(np.mean(bias_train, axis=1)),yerr=np.log10(np.std(bias_train, axis=1)), label=\"Bias Train\",        marker = 'x')\n",
    "ax.errorbar(ps, np.log10(np.mean(bias_test, axis=1)), yerr=np.log10(np.std(bias_test, axis=1)),  label=\"Bias Test\",         marker = 'x') \n",
    "ax.errorbar(ps, np.log10(np.mean(var_train, axis=1)), yerr=np.log10(np.std(var_train, axis=1)),  label=\"$\\\\sigma^2$ Train\", marker = '^')\n",
    "ax.errorbar(ps, np.log10(np.mean(var_test, axis=1)),  yerr=np.log10(np.std(var_test, axis=1)),   label=\"$\\\\sigma^2$ Test\",  marker = '^') \n",
    "ax.legend()\n",
    "ax.set_ylabel(\"log10(MSE)\")\n",
    "ax.set_xticks(ps, ps)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eafda5",
   "metadata": {},
   "source": [
    "## For profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1351c0",
   "metadata": {},
   "source": [
    "# CV OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663eea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(data, xvec, k, p, method, lmbda = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "\n",
    "    data - data to be fitted, ndarray\n",
    "    k - number of folds, int\n",
    "    p - polynomial degree, int\n",
    "\n",
    "    Out:\n",
    "\n",
    "\n",
    "    mses - MSEs for each train test pair, np array with dim = (k,)\n",
    "\n",
    "    \"\"\"    \n",
    "    # b is flat array of data to fit\n",
    "    b = data.ravel()\n",
    "    indices = np.arange(len(b.ravel()))\n",
    "    isplit = np.full(k, len(b.ravel()) // k, dtype = int)\n",
    "    isplit[:len(b.ravel()) % k] += 1\n",
    "    isplit = np.cumsum(isplit)\n",
    "    np.random.shuffle(indices)\n",
    "    splits = np.split(np.arange(len(b.ravel()))[indices], isplit[:-1])\n",
    "    # initiating the inferences\n",
    "    mses_train = np.zeros((k))\n",
    "    mses_test = np.zeros((k))\n",
    "    bias_train = np.zeros((k))\n",
    "    bias_test = np.zeros((k))\n",
    "    var_train = np.zeros((k))\n",
    "    var_test = np.zeros((k))\n",
    "    for itest in range(k):\n",
    "        test = splits[np.array(itest, dtype = int)]\n",
    "        train = np.concatenate(np.delete(splits, np.array(itest, dtype=int), axis=0))\n",
    "        #Xtrain = make_design_matrix(xvec = np.array([x.ravel()[train] for x in xvec]), p = p)\n",
    "        Xtrain = create_X(x = xvec[0].ravel()[train], y = xvec[1].ravel()[train], n = p)\n",
    "        #Xtest = make_design_matrix(xvec = np.array([x.ravel()[test] for x in xvec]), p = p)\n",
    "        Xtest = create_X(x = xvec[0].ravel()[test], y = xvec[1].ravel()[test], n = p)\n",
    "        if method == \"ols\":\n",
    "            data_tilde_train, betahat = ols_fp_wo_split(X = Xtrain, y = b[train] - np.mean(b[train]))\n",
    "            data_tilde_train += np.mean(b[train])\n",
    "            data_tilde_test = Xtest@betahat + np.mean(b[train])\n",
    "        if method == \"lasso\":\n",
    "            if lmbda is None:\n",
    "                raise ValueError(\"Lambda value has not been set\")\n",
    "            clf_train = linear_model.Lasso(alpha = lmbda)\n",
    "            clf_train.fit(X=Xtrain, y = b[train]-np.mean(b[train]))\n",
    "            betahat = clf_train.coef_\n",
    "            data_tilde_train = Xtrain@betahat + np.mean(b[train])\n",
    "            data_tilde_test = Xtest@betahat + np.mean(b[train])\n",
    "        if method == \"ridge\":\n",
    "            if lmbda is None:\n",
    "                raise ValueError(\"Lambda value has not been set\")\n",
    "            data_tilde_train, betahat = ridge_fp_wo_split(X=Xtrain, y=b[train]-np.mean(b[train]), lmbda=lmbda)\n",
    "            data_tilde_train += np.mean(b[train])\n",
    "            data_tilde_test = Xtest@betahat + np.mean(b[train])\n",
    "        mses_train[itest] = MSE(y = b[train], ytilde = data_tilde_train)\n",
    "        mses_test[itest]  = MSE(y = b[test], ytilde = data_tilde_test)\n",
    "        bias_train[itest] = bias(y = b[train], ytilde = data_tilde_train)\n",
    "        bias_test[itest]  = bias(y = b[test], ytilde = data_tilde_test)\n",
    "        var_train[itest]  = np.var(data_tilde_train)\n",
    "        var_test[itest]   = np.var(data_tilde_test)\n",
    "\n",
    "    return mses_train, mses_test, bias_train, bias_test, var_train, var_test\n",
    "   \n",
    "model_complexity = 5\n",
    "xvec = np.array([x1[::10,::10],y1[::10,::10]])\n",
    "k = 6\n",
    "mses_train, var_train, bias_train, variance_train = np.zeros((model_complexity, k)), np.zeros((model_complexity,k)),np.zeros((model_complexity, k)), np.zeros((model_complexity,k))\n",
    "mses_test, var_test, bias_test, variance_test = np.zeros((model_complexity, k)), np.zeros((model_complexity, k)),np.zeros((model_complexity, k)), np.zeros((model_complexity, k))\n",
    "p = model_complexity + 1 \n",
    "%lprun -f cross_validation cross_validation(data = terrain1[::10,::10], xvec = xvec, k = k, p = p, method = \"ridge\", lmbda = 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac2fb9",
   "metadata": {},
   "source": [
    "### OLS cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8533b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "model_complexity = 10\n",
    "xvec = np.array([x1,y1])\n",
    "scale = 1/np.sqrt(np.sum(terrain1**2))\n",
    "k = 6\n",
    "mses_train_ols, var_train_ols, bias_train_ols = np.zeros((model_complexity, k)), np.zeros((model_complexity,k)),np.zeros((model_complexity, k))\n",
    "mses_test_ols,  var_test_ols,  bias_test_ols = np.zeros((model_complexity, k)), np.zeros((model_complexity, k)),np.zeros((model_complexity, k))\n",
    "for p in range(1,model_complexity+1):\n",
    "    print(p)\n",
    "    mses_train_ols[p-1], mses_test_ols[p-1], bias_train_ols[p-1], bias_test_ols[p-1], var_train_ols[p-1], var_test_ols[p-1] = cross_validation(data = scale*terrain1, xvec = xvec, k = k, p = p, method = \"ols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2462f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(5,5), sharey=True)\n",
    "ps = np.arange(model_complexity) + 1\n",
    "ax.plot(ps, np.log10((1/scale)*np.mean(mses_train_ols, axis=1)), ls = '--', label=\"MSE Train\")\n",
    "ax.plot(ps, np.log10((1/scale)*np.mean(mses_test_ols, axis=1)), label=\"MSE Test\") \n",
    "ax.plot(ps, np.log10((1/scale)*np.mean(bias_train_ols, axis=1)), ls = '--', label=\"Bias Train\")\n",
    "ax.plot(ps, np.log10((1/scale)*np.mean(bias_test_ols, axis=1)), label=\"Bias Test\") \n",
    "ax.plot(ps, np.log10((1/scale)*np.mean(var_train_ols, axis=1)), ls = '--', label=\"Var Train\")\n",
    "ax.plot(ps, np.log10((1/scale)*np.mean(var_test_ols, axis=1)), label=\"Var Test\") \n",
    "#ax.set_xscale('log')\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"log10(MSE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cadf8c",
   "metadata": {},
   "source": [
    "## Ridge CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8cfe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "model_complexity = 10\n",
    "xvec = np.array([x1,y1])\n",
    "k = 6\n",
    "lambdas = np.logspace(-5,5, 10)\n",
    "ll = len(lambdas)\n",
    "mses_train_ridge, var_train_ridge, bias_train_ridge = np.zeros((model_complexity, ll,k)), np.zeros((model_complexity, ll,k)),np.zeros((model_complexity,  ll,k))\n",
    "mses_test_ridge, var_test_ridge, bias_test_ridge = np.zeros((model_complexity,  ll,k)), np.zeros((model_complexity,  ll,k)),np.zeros((model_complexity,  ll,k))\n",
    "for p in range(1,model_complexity+1):\n",
    "    print(p)\n",
    "    for i in range(len(lambdas)):\n",
    "        print(p, i)\n",
    "        mses_train_ridge[p-1, i], mses_test_ridge[p-1, i], bias_train_ridge[p-1, i], bias_test_ridge[p-1, i], var_train_ridge[p-1, i], var_test_ridge[p-1, i] = cross_validation(data = terrain1, xvec = xvec, k = k, p = p, method = \"ridge\", lmbda = lambdas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(model_complexity//2):\n",
    "    fig, ax = plt.subplots(1,1,figsize=(5,5), sharey=True)\n",
    "    ax.plot(lambdas, np.log10(np.mean(mses_train_ridge[i], axis=1)), ls = '--', label=\"MSE Train\")\n",
    "    ax.plot(lambdas, np.log10(np.mean(mses_test_ridge[i], axis=1)), label=\"MSE Test\") \n",
    "    ax.plot(lambdas, np.log10(np.mean(bias_train_ridge[i], axis=1)), ls = '--', label=\"Bias Train\")\n",
    "    ax.plot(lambdas, np.log10(np.mean(bias_test_ridge[i], axis=1)), label=\"Bias Test\") \n",
    "    ax.plot(lambdas, np.log10(np.mean(var_train_ridge[i], axis=1)), ls = '--', label=\"Var Train\")\n",
    "    ax.plot(lambdas, np.log10(np.mean(var_test_ridge[i], axis=1)), label=\"Var Test\") \n",
    "    ax.set_xscale('log')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(\"log10(MSE)\")\n",
    "    ax.set_ylim(4,5.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64822712",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(model_complexity//2):\n",
    "    fig, ax = plt.subplots(1,1,figsize=(5,5), sharey=True)\n",
    "    ax.plot(lambdas, np.log10(np.mean(mses_train_ridge[i], axis=1)), ls = '--', label=\"MSE Train\")\n",
    "    ax.plot(lambdas, np.log10(np.mean(mses_test_ridge[i], axis=1)), label=\"MSE Test\") \n",
    "    ax.plot(lambdas, np.log10(np.mean(bias_train_ridge[i], axis=1)), ls = '--', label=\"Bias Train\")\n",
    "    ax.plot(lambdas, np.log10(np.mean(bias_test_ridge[i], axis=1)), label=\"Bias Test\") \n",
    "    ax.plot(lambdas, np.log10(np.mean(var_train_ridge[i], axis=1)), ls = '--', label=\"Var Train\")\n",
    "    ax.plot(lambdas, np.log10(np.mean(var_test_ridge[i], axis=1)), label=\"Var Test\") \n",
    "    ax.set_xscale('log')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(\"log10(MSE)\")\n",
    "    ax.set_ylim(4,5.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce56c8",
   "metadata": {},
   "source": [
    "# Lasso CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib; import project1; importlib.reload(project1); from project1.project1 import *\n",
    "model_complexity = 5\n",
    "xvec = np.array([x1,y1])\n",
    "k = 5\n",
    "lambdas = np.logspace(-5,5, 10)\n",
    "ll = len(lambdas)\n",
    "mses_train_lasso, var_train_lasso, bias_train_lasso = np.zeros((model_complexity, ll,k)), np.zeros((model_complexity, ll,k)),np.zeros((model_complexity,  ll,k))\n",
    "mses_test_lasso, var_test_lasso, bias_test_lasso = np.zeros((model_complexity,  ll,k)), np.zeros((model_complexity,  ll,k)),np.zeros((model_complexity,  ll,k))\n",
    "for p in range(1,model_complexity+1):\n",
    "    print(p)\n",
    "    for i in range(len(lambdas)):\n",
    "        print(p, i)\n",
    "        mses_train[p-1, i], mses_test[p-1, i], bias_train[p-1, i], bias_test[p-1, i], var_train[p-1, i], var_test[p-1, i] = cross_validation(data = terrain1, xvec = xvec, k = k, p = p, \n",
    "                                                                                                                                             method = \"lasso\", lmbda = lambdas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6169927",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,ll, figsize=(ll*5,5))\n",
    "\n",
    "ps = np.arange(model_complexity)+1\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    if lambdas[i] == 0:\n",
    "        axs[i].set_title(\"$\\\\lambda = 0$\")\n",
    "    else:    \n",
    "        axs[i].set_title(\"$\\\\lambda = 10^{%.3f}$\"%np.log10(lambdas[i]))\n",
    "    axs[i].plot(ps, np.log10(np.mean(mses_train[:,i],axis=1)),  label=\"MSE train\", color = 'b')\n",
    "    axs[i].plot(ps, np.log10(np.mean(mses_test[:,i], axis=1)),  label=\"MSE test\",  color = 'b', ls='--')\n",
    "    axs[i].plot(ps, np.log10(np.mean(bias_train[:,i],axis=1)),  label=\"Bias train\",color = 'C1')\n",
    "    axs[i].plot(ps, np.log10(np.mean(bias_test[:,i], axis=1)),  label=\"Bias test\", color = 'C1', ls='--')\n",
    "    axs[i].plot(ps, np.log10(np.mean(var_train[:,i], axis=1)),  label=\"Var train\", color = 'C2')\n",
    "    axs[i].plot(ps, np.log10(np.mean(var_test[:,i],  axis=1)),  label=\"Var test\",  color = 'C2', ls='--')\n",
    "[ax.set_ylabel(\"log10(MSE)\") for ax in axs]\n",
    "[ax.set_xlabel(\"Polynomial degree\") for ax in axs]\n",
    "[ax.legend() for ax in axs]\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    fig, ax = plt.subplots(1,1,figsize=(5,5), sharey=True)\n",
    "    ax.plot(lambdas, np.log10(np.mean(mses_train[i], axis=1)), ls = '--', label=\"MSE Train\")\n",
    "    ax.plot(lambdas, np.log10(np.mean(mses_test[i], axis=1)), label=\"MSE Test\") \n",
    "    ax.plot(lambdas, np.log10(np.mean(bias_train[i], axis=1)), ls = '--', label=\"Bias Train\")\n",
    "    ax.plot(lambdas, np.log10(np.mean(bias_test[i], axis=1)), label=\"Bias Test\") \n",
    "    ax.plot(lambdas, np.log10(np.mean(var_train[i], axis=1)), ls = '--', label=\"Var Train\")\n",
    "    ax.plot(lambdas, np.log10(np.mean(var_test[i], axis=1)), label=\"Var Test\") \n",
    "    ax.set_xscale('log')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(\"log10(MSE)\")\n",
    "    ax.set_ylim(4,5.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cbcfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
